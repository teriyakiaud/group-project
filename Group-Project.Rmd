---
title: "Cluster Analysis"
authors: Aarti Arora, Andrew Mark, Natalie Robinson, Audrey Tjahjadi
output: 
  html_document: 
    toc: TRUE
    toc_float: TRUE
---

# Cluster Analysis

## Preliminaries

* Install these packages in R: {rattle.data}, {NbClust}, {flexclust}, {curl}

## Objectives
In this module we will learn about k-means clustering, the pros and cons of this method, and why its application is useful. 
(add more once we actually know what we're doing)

## Introduction {.tabset}
### What is Cluster Analysis?

Cluster analysis uncovers subgroups of observations within a dataset by reducing a large number of observations to a smaller number of **clusters**. A cluster is a group of observations that are more similar to each other than they are to the observations in other groups. 

Two popular clustering approaches are **hierarchical agglomerative clustering** and **partitioning clustering**. 

  1. Hierarchical agglomerative clustering begins with each observation as a cluster. Clusters are then combines until they all merge into a single cluster. Common algorithms include single linkage, complete linkage, average linkage, centroid, and Ward's method.
  
  2. Partitioning clustering begins with a specific K which indicated the desired number of clusters. Observations are then divided into cohesive clusters. Common algorithms include k-means and partitioning around medoids (PAM). 

Today we will only focus on k-means clustering.  
  
### K-means Clustering
  
To understand the k-means algorithm conceptually, our textbook "R in Action" provides us with the following steps:
  
    1. Select K centroids (K rows chosen at random)
    2. Assign each data point to its closest centroid
    3. Recalculate the centroids as the average of all data points in a cluster (that is,
the centroids are p-length mean vectors, where p is the number of variables).
    4. Assign data points to their closest centroids.
    5. Continue steps 3 and 4 until the observations arenâ€™t reassigned or the maximum number of iterations (R uses 10 as a default) is reached.
  
To do this, R uses an algorithm that partitions observations into k groups so that the sum of squares of the observations to their assigned cluster centers is a minimum. Each observation is assigned to the cluster with the smallest value of:

$$ ss(k) = \sum_{i=1}^n \sum_{j=0}^p (x_{ij} - \bar{x}_{kj})^2 $$
k = cluster
x_{ij} = the value of the jth variable for the ith observation
\bar{x}_{kj} = the mean of the jth variable for the kth cluster
p = the number of variables

Each of the cluster analysis methods provide different advantages. K-means is able to handle larger datasets, and observations are not permanently assigned to a specific cluster. However, because the mean is used, all variables must be continuous. If working with categorical data, another method must be used. Additionally, outliers can greatly affect results. K-means also performs poorly with non-convex clusters.
  
### Why is this useful?
  (tab content)
  
### Recent Research and Applications

Cluster analysis is commonly used in the biolgical and behavioral sciences, marketing, and medical research. 
  
Check out some of these examples:
  
https://ac-els-cdn-com.ezproxy.bu.edu/S0920410516300742/1-s2.0-S0920410516300742-main.pdf?_tid=1212e7cc-c582-11e7-afd1-00000aab0f27&acdnat=1510254990_e2794bdbec581af54de5c11ea7d5f23d
  


## Useful Skills You'll Learn Today
  * add to this at the end
  
## Example 1 (Wine Data using R in Action)

**This is the example from the R in Action K-means analysis; please consult the R in Action, 16.4.1 (pages 378-382). **

```{r}
install.packages("rattle.data") #this package contains the wine dataset 
```
R in Action asks you to download the rattle package however the wine dataset is no longer in the rattle package

```{r}
library(rattle.data)
```

```{r}
data(wine, package="rattle.data")
head(wine)
df<-scale(wine[-1]) #this new dataframe contains a standardized dataset 
```

```{r}
wssplot <- function(data, nc=15, seed=1234){ #where data is the dataset, 
#nc is the maximum number of clusters to consider, and seed is the randomly generated dataset
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}
```

K means, unlike other methods of clustering, requires that the user specify the number of clusters to make. Instead of picking a random number of clusters, one can look at a bend in a graph to determine the number of clusters to use. A graph similar to a Cattell Scree test, which is used in Principle Components Analysis and Kabacoff suggests using the NbClust package as a guide for determing the number of clusters. Kabacoff also suggests plotting the following: total within-groups sum of squares against number of clusters in the k-means solution. More information about a Scree test and an example of one is in R in Action 14.2.1. 

```{r}
wssplot(df)
library(NbClust)
set.seed(1234)
devAskNewPage(ask=TRUE)
```

```{r}
nc<-NbClust(df, min.nc=2, max.nc=15, method="kmeans") #this is how the number of clusters is determined
table(nc$Best.n[1, ])
```

```{r, eval = FALSE}
barplot(table(nc$Best.n[1,])
        xlab="Number of Clusters", ylab = "Number of Criteria", 
        main= "Number of Clusters Chosen by 26 Criteria")
#this particular section isn't really working either
```

```{r}
set.seed(1234)
fit.km<-kmeans(df, 3, nstart=25)
fit.km$size
#this is the k means clusters analysis 
fit.km$centers
```

```{r}
#cross-tabulation of Type and cluster membership:
ct.km<-table(wine$Type, fit.km$cluster)
ct.km
#agreement given by index of -1 to 1
library(flexclust)
randIndex(ct.km)
```
<<<<<<< HEAD
## Example 2 Seed Data

**Seed Data File**
=======

## Example 2 (Seed Data?)
###Seed Data File
>>>>>>> 0eaa1f0b3cce381dee5d1ba00340b92974ec25bc

This dataset contains 7 measurements for geometric properties of 210 kernels from 3 different varieties of wheat.

```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/teriyakiaud/group-project/master/tabseeddata.csv")
seeddata<-read.csv(file = f, header = TRUE, sep = ",")
head(seeddata)
ds<- scale(seeddata[-1])
```

```{r}
wssplot <- function(data, nc=15, seed=1234){ 
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
  }
```

```{r}
wssplot(ds)
library(NbClust)
set.seed(1234)
devAskNewPage(ask=TRUE)
```

As in the wine data, we can see a clear decrease in the within group sum of squares between 1, 2, and 3 clusters and then little decrease after. This suggests that 3 is an appropriate number of clusters. The figure also shows that there will most likely not be more than 15 clusters.

<<<<<<< HEAD
```{r}
nc<-NbClust(df, min.nc=2, max.nc=15, method="kmeans") 
table(nc$Best.n[1, ])
```


=======
>>>>>>> 21e4c765ceeb58123b203b02ad32d0d226f2aa7f
## Additional Resources and Useful Links
  * The Book of R Chapter 16: Cluster Analysis

  * M. Charytanowicz, J. Niewczas, P. Kulczycki, P.A. Kowalski, S. Lukasik, S. Zak, 'A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images', in: Information Technologies in Biomedicine, Ewa Pietka, Jacek Kawa (eds.), Springer-Verlag, Berlin-Heidelberg, 2010, pp. 15-24. 
