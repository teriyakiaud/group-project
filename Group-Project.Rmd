---
title: "Cluster Analysis"
authors: Aarti Arora, Andrew Mark, Natalie Robinson, Audrey Tjahjadi
output: 
  html_document: 
    toc: TRUE
    toc_float: TRUE
---

# Cluster Analysis
## Preliminaries

* Install these packages in R: {rattle.data}, {NbClust}, {flexclust}, {curl}

## Objectives
In this module we will learn about k-means clustering, the pros and cons of this method, and why its application is useful. 
(add more once we actually know what we're doing)

## Introduction {.tabset}
### What is Cluster Analysis?

Cluster analysis uncovers subgroups of observations within a dataset by reducing a large number of observations to a smaller number of **clusters**. A cluster is a group of observations that are more similar to each other than they are to the observations in other groups. 

There are three main issues with cluster analysis: (1) there are essentially an infinite number of ways the data can be clustered, (2) there is no accepted theoretical framework about how to perform cluster analyis, (3) and there is no strict definition of what a cluster is (the users define what a cluster means for their specific situation). This means that users should know what they want to understand from their data and then choose the approprite type of cluster analysis to answer those specific questions. 

Two popular clustering approaches are **hierarchical agglomerative clustering** and **partitioning clustering**. 

  1. Hierarchical agglomerative clustering begins with each observation as a cluster. Clusters are then combines until they all merge into a single cluster. Common algorithms include single linkage, complete linkage, average linkage, centroid, and Ward's method.
  
  2. Partitioning clustering begins with a specific K which indicated the desired number of clusters. Observations are then divided into cohesive clusters. Common algorithms include k-means and partitioning around medoids (PAM). 

Today we will only focus on k-means clustering.  
  
### K-means Clustering
![](http://www.turingfinance.com/wp-content/uploads/2015/02/K-Means-Clustering-Gif.gif)
  
To understand the k-means algorithm conceptually, our textbook "R in Action" provides us with the following steps:
  
1. Select K centroids (K rows chosen at random)
2. Assign each data point to its closest centroid
3. Recalculate the centroids as the average of all data points in a cluster (the centroids are p-length mean vectors where p is the number of variables).
4. Assign data points to their closest centroids.
5. Continue steps 3 and 4 until the observations aren’t reassigned or the maximum number of iterations (R uses 10 as a default) is reached.
  
To do this, R uses an algorithm that partitions observations into k groups so that the sum of squares of the observations to their assigned cluster centers is a minimum. Each observation is assigned to the cluster with the smallest value of:

$$ ss(k) = \sum_{i=1}^n \sum_{j=0}^p (x_{ij} - \bar{x}_{kj})^2 $$
k = cluster
x_{ij} = the value of the jth variable for the ith observation
\bar{x}_{kj} = the mean of the jth variable for the kth cluster
p = the number of variables

####Pros
K-means analysis is efficient, simple, robust, and can be adapted to data with complex properties, high dimentionality, and class imbalance. It can also handle larger datasets than other clustering techniques, and observations  are not permanently assigned to a specific cluster. 
 
 ####Cons
Variables for K-means must be continuous; if you are working with categorical data, another method must be used. Additionally, outliers can greatly affect results. K-means also performs poorly with convex clusters (for example, a binomial cluster), and with non-globular clusters. The results of K-means is a set of clusters which are generally uniform in size even if the original data had clusters of different sizes. 
  
### Why is this useful?
  K-means is a simple, efficient, and widely accepted method to partion data into meaningful groups; K-means is used as a benchmark to test newly created clustering techniques. 
  
### Recent Research and Applications

Cluster analysis is commonly used in the biolgical and behavioral sciences, marketing, and medical research. 
  
Check out some of these examples:
  
[Huaijie Yang, Heping Pan, Huolin Ma, Ahmed Amara Konaté, Jing Yao, Bo Guo. 2016. Performance of the synergetic wavelet transform and modified K-means clustering in lithology classification using nuclear log. Journal of Petroleum Science and Engineering, 144:1-9.](https://www.sciencedirect.com/science/article/pii/S0920410516300742)
  


## Useful Skills You'll Learn Today
  * add to this at the end
  
## Example 1 (Wine Data using R in Action)
![](https://media.giphy.com/media/3SB4hYwiSpIkw/giphy.gif)

**This is the example from the R in Action K-means analysis; please consult the R in Action, 16.4.1 (pages 378-382). **

install.packages("rattle.data") #this package contains the wine dataset 

R in Action asks you to download the rattle package however the wine dataset is no longer in the rattle package

```{r}
library(rattle.data)
```

```{r}
data(wine, package="rattle.data")
head(wine)
df<-scale(wine[-1]) #this new dataframe contains a standardized dataset 
```

```{r}
wssplot <- function(data, nc=15, seed=1234){ #where data is the dataset, 
#nc is the maximum number of clusters to consider, and seed is the randomly generated dataset
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}
```

K means, unlike other methods of clustering, requires that the user specify the number of clusters to make. Instead of picking a random number of clusters, one can look at a bend in a graph to determine the number of clusters to use. A graph similar to a Cattell Scree test, which is used in Principle Components Analysis and Kabacoff suggests using the NbClust package as a guide for determing the number of clusters. Kabacoff also suggests plotting the following: total within-groups sum of squares against number of clusters in the k-means solution. More information about a Scree test and an example of one is in R in Action 14.2.1. 

```{r}
wssplot(df)
library(NbClust)
set.seed(1234)
devAskNewPage(ask=TRUE)
```
The NbClust package allows the user to determine the optimum number of clusters for a particular dataset and provides different clustering schemes. Clusters can be determined for several methods; in this case we are using the kmeans method, however seven other methods are available (including average, median, centroid, etc.). NbClust can also select the best clustering method for a particular dataset if you, for example, want to maximize the difference between hierarchy members. 
```{r}
nc<-NbClust(df, min.nc=2, max.nc=15, method="kmeans") #this is how the number of clusters is determined
table(nc$Best.n[1, ])
```

```{r, eval = FALSE}
barplot(table(nc$Best.n[1, ]),
        xlab= "Number of Clusters", ylab = "Number of Criteria", 
        main= "Number of Clusters Chosen by 26 Criteria")
#this particular section isn't really working either
```

```{r}
set.seed(1234)
fit.km<-kmeans(df, 3, nstart=25)
fit.km$size
#this is the k means clusters analysis 
fit.km$centers
```

```{r}
#cross-tabulation of Type and cluster membership:
ct.km<-table(wine$Type, fit.km$cluster)
ct.km
#agreement given by index of -1 to 1
library(flexclust)
randIndex(ct.km)
```
<<<<<<< HEAD
## Example 2 Seed Data
![](https://i.imgur.com/cuW8K7N.gif)

**Seed Data File**
=======

## Example 2 (Seed Data)
###Seed Data File

This dataset contains 7 measurements for geometric properties of 210 kernels from 3 different varieties of wheat.

```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/teriyakiaud/group-project/master/tabseeddata.csv")
seeddata<-read.csv(file = f, header = TRUE, sep = ",")
head(seeddata)
ds<- scale(seeddata[-1])
```

```{r}
plot1 <- function(data = ds, nc=15, seed=1234){ 
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
  }
```

```{r}
plot1(ds)
library(NbClust)
set.seed(1234)
devAskNewPage(ask=TRUE)
```

As in the wine data, we can see a clear decrease in the within group sum of squares between 1, 2, and 3 clusters and then little decrease after. This suggests that 3 is an appropriate number of clusters. The figure also shows that there will most likely not be more than 15 clusters.

<<<<<<< HEAD
```{r}
nc1<-NbClust(ds, min.nc=2, max.nc=15, method="kmeans") 
table(nc1$Best.n[1, ])
```

```{r}
barplot(table(nc1$Best.n[1, ]),
        xlab="Number of Clusters", ylab = "Number of Criteria", 
        main= "Number of Clusters Chosen by 26 Criteria")
```
<<<<<<< HEAD
```{r}
set.seed(1234)
fit.km1<-kmeans(ds, 3, nstart=25)
fit.km1$size
fit.km1$centers
```
=======

```{r}
library(knitr)
```

>>>>>>> a06f47aa36b926147f4015312676b203e9daaa88
=======

## Additional Resources and Useful Links
  * The Book of R Chapter 16: Cluster Analysis

  * [M. Charytanowicz, J. Niewczas, P. Kulczycki, P.A. Kowalski, S. Lukasik, S. Zak, 'A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images', in: Information Technologies in Biomedicine, Ewa Pietka, Jacek Kawa (eds.), Springer-Verlag, Berlin-Heidelberg, 2010, pp. 15-24.](https://www.researchgate.net/publication/234073581_Seeds_dataset) 
  
  *https://cran.r-project.org/web/packages/NbClust/NbClust.pdf
  
  * [Wu, Junjie. *Advances in K-means clustering a data mining thinking.* Springer, New York 2012.](https://link.springer.com/book/10.1007%2F978-3-642-29807-3) 
